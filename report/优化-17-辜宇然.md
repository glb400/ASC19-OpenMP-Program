# 超算优化选拔题

【个人信息】

17343031 辜宇然 软工一班

【报告】

本报告分为2个部分：

1. 优化组必做题
2. 系统组

【正文】

## part1 优化组必做题

**优化思路**

安装xshell+xftp进行远程控制，利用vtune等软件检测分析程序性能瓶颈，找到慢的部分使用并行方法修改程序，或者使用向量化，内存访问优化。

**编译运行**

首先使用xshell进行远程连接，之后首先连接到i1：

```
ssh -p 12345 u17343031@fhk.asmodeus.cn
```

然后使用xftp将FYArray.tar.tgz从本地上传到i1：

![](./localtoi1.png)

然后使用scp命令将FYArray推送到knl02的对应用户节点（只能进行i1到knl的单向连接）：

![](./i1toknl.png)

最后安装intel编译器环境：

```
source /opt/intel/parallel_studio_xe_2018.1.038/psxevars.sh
```

最后初次运行结果为：83.6144s

![](./res1.png)

**每一步优化结果**

首先使用vtune进行检测：

vtune用于分析和优化程序性能，用于**针对寻找软硬件性能瓶颈**，**确定程序的热点（hotspot）**，**找到导致性能不理想的原因**。

按照提示将FLAGS变量修改为：

```
-qopenmp -g -O2
```

![](./makefile.png)

接下来使用命令：

```
amplxe-cl -collect hotspots ./FYArray.exe
```

得到文件r000hs：

![](./amplxe.png)

![](./amplxe2.png)

最后在i1节点使用scp本地下载（命令只是参数与上传相反）有：

![](./download.png)

接下来使用vtune进行分析：

![](./vtune.png)

可以看到，占用时间最多的就是main，于是针对该文件进行修改。

进入main.cpp可以看到所有的运行时间都是不理想的，需要针对这些部分进行优化，其中需要重点优化的部分为：

![](./vtune1.png)

![](./vtune2.png)

---

**接下来使用openmp进行优化**：

可以看到目前仅使用了一个线程：

![](./vtune3.png)

可以看到此时CPU time = 150.880s

**关于openmp**

首先学习如何使用openmp进行并行编程，就先学习openmp的结构。其结构包括：

+ 并行控制
+ 工作共享
+ 数据环境
+ 线程同步
+ 环境变量
+ 库函数

其中我们在各个步骤需要完成的是分别为：

在并行控制中**设置并行区域并创建线程组**；在工作共享中**针对线程分配任务或者进行向量化（位于并行域内）**；数据环境负责**并行域内的变量属性和边界的数据传输**；线程同步通过互斥锁**控制线程的执行顺序从而保证结果的正确**；库函数和环境变量用来设置和获取执行环境相关的信息。

重点说，omp是将工作划分为多个子任务分配给多个线程，从而实现多核并行处理单一的地址空间。但是使用omp会出现一些并发问题需要解决。

了解omp后下面分三次进行优化：

+ omp多线程优化
+ SIMD向量化优化
+ 存储使用的优化


**第一次优化:parallel**

【理论基础】

&emsp;&emsp;第一次优化是使用omp进行并发编程。根据vtune分析，大量时间消耗在循环计算。因此使用指令for对循环进行并行化处理。还可以使用指令sections，如果将一个线程执行的长for循环分割成几部分让多个线程同时执行，也可以节约计算时间。

&emsp;&emsp;这种优化属于并行构造，其使用工作共享指令。工作共享指令负责任务划分，并分发给各个线程。并行构造指令包括并行区域指令和工作共享指令两部分。在使用并行构造指令后，整个工作区域被分割成多个可执行的工作分区。线程组内各个线程自动地从各个工作分区中获取任务执行，每个子线程在执行完毕当前工作分区后，如果工作分区存在完成的工作分区，则会继续获取任务执行。

&emsp;&emsp;总体来说，并行构造包括并行区域指令(parallel)和工作共享指令，这里主要用规则循环下使用的指令(for/single/task/simd)，最后使用负载均衡。负载均衡主要有静态和动态，这里暂且打算使用循环调度（根据余弦规定的次序，逐个给线程分配计算任务，分配完的线程放在最后）（若发现负载不均衡的话）。

&emsp;&emsp;大概明确步骤后，了解一下具体的并行分配任务，影响并行执行的一个重要因素是程序中的依赖关系。我们在优化时，因为是针对每一个循环计算进行并行化，因此没有内存依赖的问题。

关于循环依赖，以下面一个多重循环为例可以看到循环计算没有循环依赖现象：

![](./iterate.png)

实际上看过该文件所有的多重循环计算后发现都没有循环依赖现象。

关于任务依赖，我们使用形如下述的结构：

```c++
#pragma omp parallel sections
{
    #pragma omp sections
    {
        //part1
    }
    #pragma omp sections
    {
        //part2
    }
}
```

【下面开始对多重循环上omp】

使用
```c++
#pragma omp for 
```
对循环结构处理，该指令表明接下来的循环将被并行执行（前提是该并行区域已经被指令parallel初始化）。

**其工作流程如下：**
```
利用parallel指令将N次循环置于并行域内
->
利用for指令将N次分配循环的任务分配，即将这N次循环的计算工作量进行任务划分
—>
让每个线程各自负责其中的一部分循环工作，因此必须保证每次循环之间数据的相互独立性。（循环依赖）
```

**for指令的具体使用方法：**
```c++
#pragma omp for[子句...]
    schedule(类型[,循环迭代次数]) //设置循环迭代次数的分配策略
    ordered
    private(variable list)
    firstprivate(variable list)
    lastprivate(variable list)
    shared(variable list)
    reduction(运算符:variable list) //规约运算
    collapse(n) //指定在一个嵌套循环中的n重循环被合并成一个更大的循环空间 + 根据schedule子句来划分循环迭代次数
    nowait //线程组线程是否同步
{
    循环体
}
```

使用for指令有很多注意事项，大致为限定for指令作用域的并行性等等我看过了就不作赘述了。下面看下怎么改：

【示例1：单重循环】

```c++
#include<omp.h>
int main(){
    omp_set_num_threads(xx);
    ...
    int nthreads,tid,i;
    #pragma omp parallel private(tid,nthreads)shared(a,b,c)
    {
        #pragma omp for
        for(i=0;i<10;i++)
        {
            tid = omp_get_thread_num();
            nthreads = omp_get_num_threads();
            c[i] = a[i] + b[i];
        }
    }
}
```

【示例2：多重循环】

```c++
#include<omp.h>
int main(){
    int nthreads,tid,i;
    omp_set_num_thread(xx);
    #pragma omp parallel for private(i,j,tid,nthreads)shared(a,b,c)
    for(i=0;i<3;i++){
        for(j=0;j<3;j++){
            a[i][j] = (i+1) + (j+1);
            b[i][j] = (i+j+2) * 10;
            c[i][j] = a[i][j] + b[i][j];
        }
    }
}
//在上述程序中因为循环并行指导语句#pragma omp parallel for在外部循环的上部，因此对外部循环进行并行执行。
```

【本题：main.cpp】

使用了一般的for指令，本代码中循环计算没有多线程竞争数据现象故不使用reduction指令

因为在xeon cpu中有超线程的概念（一个实体CPU提供两个逻辑线程），因此我们查看cpu是否开启超线程，发现超线程开启，因此将线程数设置为32。

（通过 cat /proc/cpuinfo查看）

![](./cpuinfo.png)

看到`siblings`是`cpu cores`的两倍，因此超线程开启

编译参数变为：
```shell
-qopenmp -g -O2
```

因为跑分不用调试所以变为：
```shell
-qopenmp -O2
```

发现错误，分析原因是因为在大循环中除了小循环还有别的操作，因此将并行对象修改为小循环

首次优化结果：

![](./yh1.png)

CPU time = 9.99257s

时间比为：83.6144s / 9.99257s = 8.367657169276773倍

因为发现在一个多重循环中不同的线程读写数组时不会数据竞争，因此加上collapse子句有：

![](./yh2.png)

CPU time = 9.75958s

时间比为：83.6144s / 9.75958s = 8.567417860194803倍

使用指导性优化调度(schedule(guided))，通过应用指导性的启发式自调用动态分配迭代块大小

CPU time = 9.69019s

时间比为：83.6144s / 9.69019s = 8.628767856977005倍

![](./yh3.png)

**第二次优化:section**

第一次优化是针对循环计算，那么第二次优化则针对非循环部分的分段并行(sections)。先找到程序中无先后依赖的部分然后执行sections划分。

注意原代码中245-283两段多重计算式右侧说明没有先后关联，因此可以分段并行。不过由于只选择一部分进行分段，优化效果不明显。

![](./yh4.png)

**第三次优化:线程同步**

因为前两次优化遇到瓶颈，因此优化后使用vtune分析一下性能：

![](./vtune4.png)

分析发现大量时间用于并行区域中的隐式barrier，且优化并不足够。

不过显示的是32线程，故将其改为64线程。

结果为：

![](./yh5.png)

现在进行第三次优化：

首先为了避免在循环过程中出现不必要的同步barrier，可以增加指令nowait到相应的编译指导语句中。注意到在下列语句中都存在隐式barrier：

```c++
#pragma omp parallel
#pragma omp for
#pragma omp sections
```

因此将原来的语句修改拆分为parallel + for，并在for指令后添加nowait指令。

```c++
#pragma omp parallel
{
	#pragma omp for collapse(2)schedule(guided)nowait
}
```

优化结果：

![](./yh6.png)

无明显变化。

时间比：83.6144s / 8.77584s = 9.527794490327991倍

**第四次优化:向量化**

因为没有循环依赖，因此可以使用simd向量化。向量化就是针对相同指令在simd单元上对多个数据流进行操作，它通过一些方式充分挖掘程序中的并行性，将程序中可以并行化的部分合成处理器支持的向量指令，通过复制多个操作数并将其直接打包在寄存器中，从而完成在同一时间采用同步方式对多个数据执行同一条指令。

向量化是在并行程度高的代码上，充分挖掘并行处理能力，所以我们在64线程下可以尝试这种方法。使用avx的话对于整型变量向量长度为8。在编译时-O2和-O3已包含向量化编译优化。

现在来实现向量化，**可实现向量化的循环有如下特点**，下面针对以下特点逐一验证或修改使之满足：

1. 循环内任何语句间无循环依赖（原代码满足，当然如果不满足的话可以`用safelen限制这种向后依赖的关系`【在限定向量长度safelen内循环迭代之间不存在依赖】）

2. 向量化处理的数据类型一致（原代码满足）

3. 循环指标变量必须为整数且在循环入口处循环次数确定（原代码满足，循环次数都为预定义好的常数）

4. 循环为单入口单出口（原代码满足）

5. 任何两个循环之间不得有任何代码或openmp指令（原代码满足）

6. 对数据进行合理规划，使用顺序访问提高cache命中率。也就是说，对于多维数组的读写ijk顺序内外层循环。（原代码不满足，但是观察发现可以修改顺序无影响）

7. 在循环时不使用输入输出和分支语句（原代码满足）

但是在调整顺序后发现elapsed time大大增加：

![](./simd1.png)

因为不连续的访问导致cache命中率大大降低，虽然在向量化后取得了一定程度的提升，但是连原来的效果都达不到：

![](./simd2.png)

于是分析原因，发现是因为数组系数并不是三维而是四维，在最外层还有一维for m，从而导致cache访问不连续，因此需要将其翻到内侧。

大概长这样：

![](./simd3.png)

此时优化结果为：

![](./simd4.png)

发现仍然不如向量化之前的结果，我感觉有点不对所以回去看，发现取消向量化之后速度是几乎一样的，也就是说向量化没有效果：

![](./simd5.png)

既然向量化没有效果那么编译时加上
```
-qopt-report-phase=vec
```
输出向量化报告一看：

![](./simd6.png)

几乎所有都没有被向量化......原因如下：

```
1. outer loop was not auto-vectorized: consider using SIMD directive
2. loop was not vectorized: vector dependence prevents vectorization. 
```

关于前一个问题在下述网站上可以找到答案：

https://www.jianshu.com/p/186339c16e8c

![](./simd8.png)

将编译参数改为-O3，现在可以看到仍然没有成功矢量化，但是已经原因变了：

![](./simd7.png)

最后发现矢量化的部分只能进行一层，因此选择最内层且循环次数最大的i层：

![](./simd10.png)

优化效果有限，打开向量化报告发现已经对内层循环进行了向量化，因此感觉瓶颈并不在循环部分，开始重新用vtune进行测评，此时的优化比为：

CPU time = 8.47876s

时间比为：83.6144s / 8.47876s = 9.861630710151013倍

---

## 分析vtune找出瓶颈进行优化

此前优化没有基于vtune，这次出现了瓶颈，所以使用vtune进行测评后认真分析vtune中数据，数据如下：

![](./vtune5.png)

把这次与上次对比:

![](./vtune4.png)

看到并行化程度从原来的 203.259s / 25.574s = 7.94787674982404 变为 252.893s / 23.581s = 10.72443916712608，实际上提高比较明显，但是可能在并行过程中开销比较大，所以优化不明显。注意到此次的空转时间(spin time)明显高于上次（150.829s > 63.652s），故此次优化主要放在减少spin time上。

再次查看看到集中于imbalance or serial spinning一项，查阅资料发现是指`工作线程在消耗CPU资源的同步barrier上spin时的CPU时间。这可能是由负载不平衡，所有工作线程的并发性不足或在串行化执行的情况下等待barrier引起的。`，就是并发不充分或者是等待同步barrier：

![](./vtune6.png)

看到最大的开销就是fork和隐式barrier fork_barrier，那么想到性能优化中很重要一条。那么首先将所有的parallel合并为一个大的并行区域，此时看到大部分时间为effective，有效时间提升：

![](./vtune7.png)

此时查看发现并行化部分效率显著提高，问题在于未并行化部分，因此将他们一同并行化：

![](./vtune8.png)

![](./vtune9.png)

于是最后修改代码如下：

```c++
int main()
{
	omp_set_num_threads(64);

	double start,end,elapsed;
	const int nDim = THREE_D;
	const double fourth = 0.25;
	int mst = 0;
	int med = 3;


	Range I(-1,ni+1);
	Range J(-1,nj+1);
	Range K(-1,nk+1);
	RDouble3D x(I, J, K, fortranArray);
	RDouble3D y(I, J, K, fortranArray);
	RDouble3D z(I, J, K, fortranArray);
	for ( int k = -1; k <= nk+1; ++ k )
	{
		for ( int j = -1; j <= nj+1; ++ j )
		{
			for ( int i = -1; i <= ni+1; ++ i )
			{
				x(i,j,k) = i*0.1;
				y(i,j,k) = j*0.2;
				z(i,j,k) = k*0.3;
			}
		}
	}
	

	// ÉêÇë±äÁ¿¿Õ¼ä
	I = Range(-1,ni+1);
	J = Range(-1,nj+1);
        K = Range(-1,nk+1);
        Range D(1,3);
	RDouble4D xfn (I,J,K,D,fortranArray);  // Íø¸ñµ¥Ôª¨L×óÏÂÃæ·¨Ïò£¬DÎª·½Ïò
	RDouble4D yfn (I,J,K,D,fortranArray);
	RDouble4D zfn (I,J,K,D,fortranArray);
	RDouble4D area(I,J,K,D,fortranArray);  // Íø¸ñµ¥Ôª¨L×óÏÂÃæÃæ»ý
	RDouble3D vol (I,J,K,  fortranArray);  // Íø¸ñµ¥ÔªÌå»ý

        Range M(0,3); // 4¸ö±äÁ¿£ºËÙ¶Èu¡¢v¡¢w£¬ÎÂ¶ÈT
        RDouble4D q_4d(I,J,K,M,fortranArray); // ´æ´¢Á÷³¡Á¿£¬Î»ÖÃÔÚµ¥ÔªÖÐÐÄ
	RDouble4D dqdx_4d(I,J,K,M,fortranArray); // ´æ´¢Á÷³¡Á¿¼ÆËãµÃµ½µÄÌÝ¶ÈÆ«x
	RDouble4D dqdy_4d(I,J,K,M,fortranArray); // ´æ´¢Á÷³¡Á¿¼ÆËãµÃµ½µÄÌÝ¶ÈÆ«y
	RDouble4D dqdz_4d(I,J,K,M,fortranArray); // ´æ´¢Á÷³¡Á¿¼ÆËãµÃµ½µÄÌÝ¶ÈÆ«z

	// ¼ÆËãÍø¸ñµ¥Ôª¼¸ºÎÊý¾Ý xfn¡¢fn¡¢zfn¡¢area¡¢vol
	// ËÙ¶Èu¡¢v¡¢w£¬ÎÂ¶ÈT Á÷³¡±äÁ¿¸³Öµ£¬´æ´¢ÔÚq_4dÖÐ£¬±ãÓÚºóÃæËÙ¶È¡¢ÎÂ¶È½çÃæÌÝ¶È¼ÆËã
	// ³ÌÐòÃ¿Ö´ÐÐÒ»¸öµü´ú²½£¬Á÷³¡±äÁ¿±»¸üÐÂ¡£´Ë´¦¸ø³õ³¡Öµu=1.0£¬v=0.0£¬w=0.0£¬T=1.0
	for ( int k = -1; k <= nk+1; ++ k )
	{
		for ( int j = -1; j <= nj+1; ++ j )
		{
			for ( int i = -1; i <= ni+1; ++ i )
			{
				xfn(i,j,k,1) = 1.0;
				xfn(i,j,k,2) = 0.0;
				xfn(i,j,k,3) = 0.0;
				yfn(i,j,k,1) = 0.0;
				yfn(i,j,k,2) = 1.0;
				yfn(i,j,k,3) = 0.0;
				zfn(i,j,k,1) = 0.0;
				zfn(i,j,k,2) = 0.0;
				zfn(i,j,k,3) = 1.0;
				area(i,j,k,1) = 0.06;
				area(i,j,k,2) = 0.03;
				area(i,j,k,3) = 0.02;
				vol(i,j,k) = 0.006;
			}
		}
	}
	for ( int k = -1; k <= nk+1; ++ k )
	{
		for ( int j = -1; j <= nj+1; ++ j )
		{
			for ( int i = -1; i <= ni+1; ++ i )
			{
				q_4d(i,j,k,0) = (x(i,j,k) * x(i,j,k) + y(i,j,k)*y(i,j,k)- 1.3164) / 2.1547; // u = a*x*x+b*y*y
				q_4d(i,j,k,1) = (z(i,j,k)*z(i,j,k) - 0.2157 ) * 0.137; // v=c*z*z
				q_4d(i,j,k,2) = (2.0*x(i,j,k) +  1.737) / 3.14; // w=d*x
				q_4d(i,j,k,3) = x(i,j,k) + y(i,j,k) + 1.3765; // T = x + y
			}
		}
	}
	start=rdtsc();
	//ÒÔÉÏÎªÊý¾Ý³õÊ¼»¯²¿·Ö£¬²»¿ÉÐÞ¸Ä£¡
	// --------------------------------------------------------------------
	// Çó½âËÙ¶È¡¢ÎÂ¶ÈÔÚ¡°µ¥Ôª½çÃæ¡±ÉÏµÄÌÝ¶È£¬i¡¢j¡¢kÈý¸ö·½ÏòÒÀ´ÎÇó½â
	// ÔÚ³ÌÐòÖÐÊÇ¡°ºÄÊ±²¿·Ö¡±£¬Ã¿Ò»¸öµü´ú²½¶¼»áÇó½â£¬ÒÔÏÂÎªÎ´ÓÅ»¯´úÂë
	// Ï£Íû²ÎÈü¶ÓÎéÔÚÀí½â¸ÃËã·¨µÄ»ù´¡ÉÏ£¬ÊµÏÖ¸ü¸ßÐ§µÄ½çÃæÌÝ¶ÈÇó½â£¬ÌáÉý³ÌÐòÖ´ÐÐÐ§ÂÊ
	// --------------------------------------------------------------------
	// ´Ë´¦¿ªÊ¼Í³¼Æ¼ÆËã²¿·Ö´úÂëÔËÐÐÊ±¼ä

	for ( int nsurf = 1; nsurf <= THREE_D; ++ nsurf )
	{
#define EXPANDED
#define LOC4D(i0, i1, i2, i3)	((i0) * s0 + (i1) * s1 + (i2) * s2 + (i3) * s3)
#define LOC3D(i0, i1, i2)		((i0) * s0 + (i1) * s1 + (i2) * s2)
		Range I(1,ni+1);
		Range J(1,nj+1);
		Range K(1,nk+1);

		int index[] = {1,2,3,1,2};

		int ns1 = nsurf;
		int ns2 = index[nsurf  ];
		int ns3 = index[nsurf+1];

		int il1 = 0;
		int il2 = 0;
		int il3 = 0;
		int jl1 = 0;
		int jl2 = 0;
		int jl3 = 0;
		int kl1 = 0;
		int kl2 = 0;
		int kl3 = 0;

		if ( nsurf == 1 )
		{
			il1 = 1;
			jl2 = 1;
			kl3 = 1;
		}
		else if ( nsurf == 2 )
		{
			jl1 = 1;
			kl2 = 1;
			il3 = 1;
		}
		else if ( nsurf == 3 )
		{
			kl1 = 1;
			il2 = 1;
			jl3 = 1;
		}

		Range M(mst,med);

		Range IW(-1,ni+1);
		Range JW(-1,nj+1);
		Range KW(-1,nk+1);

		RDouble3D worksx(IW,JW,KW,fortranArray);
		RDouble3D worksy(IW,JW,KW,fortranArray);
		RDouble3D worksz(IW,JW,KW,fortranArray);
		RDouble3D workqm(IW,JW,KW,fortranArray);

#ifdef EXPANDED
		double* Pworksx = &worksx[0];
		double* Pworksy = &worksy[0];
		double* Pworksz = &worksz[0];
		double* Pworkqm = &workqm[0];
		double* Pdqdx_4d = &dqdx_4d[0];
		double* Pdqdy_4d = &dqdy_4d[0];
		double* Pdqdz_4d = &dqdz_4d[0];
		double* Pxfn = &xfn[0];
		double* Pyfn = &yfn[0];
		double* Pzfn = &zfn[0];
		double* Parea = &area[0];
		double* Pq_4d = &q_4d[0];
		double* Pvol = &vol[0];

		const int s0 = 1;
		const int s1 = s0 * (ni + 3);
		const int s2 = s1 * (nj + 3);
		const int s3 = s2 * (nk + 3);
#endif

#pragma omp parallel
{
#ifdef EXPANDED
		for ( int m = mst; m <= med; ++ m ) {		
			#pragma omp for schedule(dynamic)nowait
			#pragma ivdep
			for(int k = 1; k <= nk+1; ++k) {
				for(int j = 1; j <= nj+1; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni+1; ++i) {
						Pdqdx_4d[LOC4D(i,j,k,m)] = 0.0;
						Pdqdx_4d[LOC4D(i,j,k,m)] = 0.0;
						Pdqdx_4d[LOC4D(i,j,k,m)] = 0.0;
					}
				}
			}
		}
#else
		dqdx_4d(I,J,K,M) = 0.0;
		dqdy_4d(I,J,K,M) = 0.0;
		dqdz_4d(I,J,K,M) = 0.0;
#endif

#ifdef EXPANDED
		#pragma omp for schedule(dynamic)nowait
		#pragma ivdep
		for(int k = 1; k <= nk+1; ++k) {
			for(int j = 1; j <= nj+1; ++j) {
				#pragma omp simd
				for(int i = 1; i <= ni+1; ++i) {
					Pworksx[LOC3D(i,j,k)] = \
						Pxfn[LOC4D(i,j,k,ns1)] * Parea[LOC4D(i,j,k,ns1)] + \
						Pxfn[LOC4D(i-il1,j-jl1,k-kl1,ns1)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns1)];
					Pworksy[LOC3D(i,j,k)] = \
						Pyfn[LOC4D(i,j,k,ns1)] * Parea[LOC4D(i,j,k,ns1)] + \
						Pyfn[LOC4D(i-il1,j-jl1,k-kl1,ns1)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns1)];
					Pworksz[LOC3D(i,j,k)] = \
						Pzfn[LOC4D(i,j,k,ns1)] * Parea[LOC4D(i,j,k,ns1)] + \
						Pzfn[LOC4D(i-il1,j-jl1,k-kl1,ns1)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns1)];
				}
			}
		}
#else
		worksx(I,J,K) = xfn(I,J,K,ns1) * area(I,J,K,ns1) + xfn(I-il1,J-jl1,K-kl1,ns1) * area(I-il1,J-jl1,K-kl1,ns1);
		worksy(I,J,K) = yfn(I,J,K,ns1) * area(I,J,K,ns1) + yfn(I-il1,J-jl1,K-kl1,ns1) * area(I-il1,J-jl1,K-kl1,ns1);
		worksz(I,J,K) = zfn(I,J,K,ns1) * area(I,J,K,ns1) + zfn(I-il1,J-jl1,K-kl1,ns1) * area(I-il1,J-jl1,K-kl1,ns1);
#endif

		for ( int m = mst; m <= med; ++ m )
		{
#ifdef EXPANDED
			#pragma omp for schedule(dynamic)nowait
			#pragma ivdep
			for(int k = 1; k <= nk+1; ++k) {
				for(int j = 1; j <= nj+1; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni+1; ++i) {
							Pdqdx_4d[LOC4D(i,j,k,m)] = \
								- Pworksx[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
							Pdqdy_4d[LOC4D(i,j,k,m)] = \
								- Pworksy[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
							Pdqdz_4d[LOC4D(i,j,k,m)] = \
								- Pworksz[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
					}
				}
			}
#else
			dqdx_4d(I,J,K,m) = - worksx(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
			dqdy_4d(I,J,K,m) = - worksy(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
			dqdz_4d(I,J,K,m) = - worksz(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
#endif
		}

		for ( int m = mst; m <= med; ++ m )
		{
#ifdef EXPANDED
			#pragma omp for schedule(dynamic)nowait
			#pragma ivdep 
			for(int k = 1; k <= nk+1; ++k) {
				for(int j = 1; j <= nj+1; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni+1; ++i) {
							Pdqdx_4d[LOC4D(i-il1,j-jl1,k-kl1,m)] += \
								Pworksx[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
							Pdqdy_4d[LOC4D(i-il1,j-jl1,k-kl1,m)] += \
								Pworksy[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
							Pdqdz_4d[LOC4D(i-il1,j-jl1,k-kl1,m)] += \
								Pworksz[LOC3D(i,j,k)] * Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)];
					}
				}
			}
#else		
			dqdx_4d(I-il1,J-jl1,K-kl1,m) += worksx(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
			dqdy_4d(I-il1,J-jl1,K-kl1,m) += worksy(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
			dqdz_4d(I-il1,J-jl1,K-kl1,m) += worksz(I,J,K) * q_4d(I-il1,J-jl1,K-kl1,m);
#endif
		}

		if ( ( nsurf != 2 ) || ( nDim != TWO_D ) )
		{
#ifdef EXPANDED
			#pragma omp for schedule(dynamic)nowait
			#pragma ivdep
			for(int k = 1; k <= nk+1; ++k) {
				for(int j = 1; j <= nj+1; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni+1; ++i) {
						Pworksx[LOC3D(i,j,k)] = \
							Pxfn[LOC4D(i,j,k,ns2)] * Parea[LOC4D(i,j,k,ns2)] + \
							Pxfn[LOC4D(i-il1,j-jl1,k-kl1,ns2)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns2)];
						Pworksy[LOC3D(i,j,k)] = \
							Pyfn[LOC4D(i,j,k,ns2)] * Parea[LOC4D(i,j,k,ns2)] + \
							Pyfn[LOC4D(i-il1,j-jl1,k-kl1,ns2)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns2)];
						Pworksz[LOC3D(i,j,k)] = \
							Pzfn[LOC4D(i,j,k,ns2)] * Parea[LOC4D(i,j,k,ns2)] + \
							Pzfn[LOC4D(i-il1,j-jl1,k-kl1,ns2)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns2)];
					}
				}
			}
#else
			worksx(I,J,K) = xfn(I,J,K,ns2) * area(I,J,K,ns2) + xfn(I-il1,J-jl1,K-kl1,ns2) * area(I-il1,J-jl1,K-kl1,ns2);
			worksy(I,J,K) = yfn(I,J,K,ns2) * area(I,J,K,ns2) + yfn(I-il1,J-jl1,K-kl1,ns2) * area(I-il1,J-jl1,K-kl1,ns2);
			worksz(I,J,K) = zfn(I,J,K,ns2) * area(I,J,K,ns2) + zfn(I-il1,J-jl1,K-kl1,ns2) * area(I-il1,J-jl1,K-kl1,ns2);
#endif

			for ( int m = mst; m <= med; ++ m )
			{
#ifdef EXPANDED
			    #pragma omp for schedule(dynamic)nowait
				#pragma ivdep
				for(int k = 1; k <= nk+1; ++k) {
					for(int j = 1; j <= nj+1; ++j) {
						#pragma omp simd
						for(int i = 1; i <= ni+1; ++i) {
								Pworkqm[LOC3D(i,j,k)] = fourth * ( \
									Pq_4d[LOC4D(i,j,k,m)] + Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)] + \
									Pq_4d[LOC4D(i-il2,j-jl2,k-kl2,m)] + Pq_4d[LOC4D(i-il1-il2,j-jl1-jl2,k-kl1-kl2,m)] );

								Pdqdx_4d[LOC4D(i,j,k,m)] -= \
									Pworksx[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
								Pdqdy_4d[LOC4D(i,j,k,m)] -= \
									Pworksy[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
								Pdqdz_4d[LOC4D(i,j,k,m)] -= \
									Pworksz[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
								Pdqdx_4d[LOC4D(i-il2,j-jl2,k-kl2,m)] += \
									Pworksx[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
								Pdqdy_4d[LOC4D(i-il2,j-jl2,k-kl2,m)] += \
									worksy[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
								Pdqdz_4d[LOC4D(i-il2,j-jl2,k-kl2,m)] += \
									Pworksz[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
						}
					}	
				}
#else
				workqm(I,J,K) = fourth * ( q_4d(I,J,K,m) + q_4d(I-il1,J-jl1,K-kl1,m) + q_4d(I-il2,J-jl2,K-kl2,m) + q_4d(I-il1-il2,J-jl1-jl2,K-kl1-kl2,m) );

				dqdx_4d(I,J,K,m) -= worksx(I,J,K) * workqm(I,J,K);
				dqdy_4d(I,J,K,m) -= worksy(I,J,K) * workqm(I,J,K);
				dqdz_4d(I,J,K,m) -= worksz(I,J,K) * workqm(I,J,K);

				dqdx_4d(I-il2,J-jl2,K-kl2,m) += worksx(I,J,K) * workqm(I,J,K);
				dqdy_4d(I-il2,J-jl2,K-kl2,m) += worksy(I,J,K) * workqm(I,J,K);
				dqdz_4d(I-il2,J-jl2,K-kl2,m) += worksz(I,J,K) * workqm(I,J,K);
#endif
			}
		}

		if ( ( nsurf != 1 ) || ( nDim != TWO_D ) )
		{
#ifdef EXPANDED
		    #pragma omp for schedule(dynamic)nowait
			#pragma ivdep			
			for(int k = 1; k <= nk+1; ++k) {
				for(int j = 1; j <= nj+1; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni+1; ++i) {
						Pworksx[LOC3D(i,j,k)] = \
							Pxfn[LOC4D(i,j,k,ns3)] * Parea[LOC4D(i,j,k,ns3)] + \
							Pxfn[LOC4D(i-il1,j-jl1,k-kl1,ns3)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns3)];
						Pworksy[LOC3D(i,j,k)] = \
							Pyfn[LOC4D(i,j,k,ns3)] * Parea[LOC4D(i,j,k,ns3)] + \
							Pyfn[LOC4D(i-il1,j-jl1,k-kl1,ns3)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns3)];
						Pworksz[LOC3D(i,j,k)] = \
							Pzfn[LOC4D(i,j,k,ns3)] * Parea[LOC4D(i,j,k,ns3)] + \
							Pzfn[LOC4D(i-il1,j-jl1,k-kl1,ns3)] * Parea[LOC4D(i-il1,j-jl1,k-kl1,ns3)];
					}
				}
			}
#else
			worksx(I,J,K) = xfn(I,J,K,ns3) * area(I,J,K,ns3) + xfn(I-il1,J-jl1,K-kl1,ns3) * area(I-il1,J-jl1,K-kl1,ns3);
			worksy(I,J,K) = yfn(I,J,K,ns3) * area(I,J,K,ns3) + yfn(I-il1,J-jl1,K-kl1,ns3) * area(I-il1,J-jl1,K-kl1,ns3);
			worksz(I,J,K) = zfn(I,J,K,ns3) * area(I,J,K,ns3) + zfn(I-il1,J-jl1,K-kl1,ns3) * area(I-il1,J-jl1,K-kl1,ns3);
#endif
			
			for ( int m = mst; m <= med; ++ m )
			{
#ifdef EXPANDED
				#pragma omp for schedule(dynamic)nowait
				#pragma ivdep	
				for(int k = 1; k <= nk+1; ++k) {
					for(int j = 1; j <= nj+1; ++j) {
						#pragma omp simd
						for(int i = 1; i <= ni+1; ++i) {
							Pworkqm[LOC3D(i,j,k)] = fourth * ( \
								Pq_4d[LOC4D(i,j,k,m)] + Pq_4d[LOC4D(i-il1,j-jl1,k-kl1,m)] + \
								Pq_4d[LOC4D(i-il3,j-jl3,k-kl3,m)] + Pq_4d[LOC4D(i-il1-il3,j-jl1-jl3,k-kl1-kl3,m)] );

							Pdqdx_4d[LOC4D(i,j,k,m)] -= \
								Pworksx[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
							Pdqdy_4d[LOC4D(i,j,k,m)] -= \
								Pworksy[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
							Pdqdz_4d[LOC4D(i,j,k,m)] -= \
								Pworksz[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];

							Pdqdx_4d[LOC4D(i-il3,j-jl3,k-kl3,m)] += \	
								Pworksx[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
							Pdqdy_4d[LOC4D(i-il3,j-jl3,k-kl3,m)] += \
								Pworksy[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
							Pdqdz_4d[LOC4D(i-il3,j-jl3,k-kl3,m)] += \
								Pworksz[LOC3D(i,j,k)] * Pworkqm[LOC3D(i,j,k)];
						}
					}
				}
#else
				workqm(I,J,K) = fourth * ( q_4d(I,J,K,m) + q_4d(I-il1,J-jl1,K-kl1,m) + q_4d(I-il3,J-jl3,K-kl3,m) + q_4d(I-il1-il3,J-jl1-jl3,K-kl1-kl3,m) );

				dqdx_4d(I,J,K,m) -= worksx(I,J,K) * workqm(I,J,K);
				dqdy_4d(I,J,K,m) -= worksy(I,J,K) * workqm(I,J,K);
				dqdz_4d(I,J,K,m) -= worksz(I,J,K) * workqm(I,J,K);

				dqdx_4d(I-il3,J-jl3,K-kl3,m) += worksx(I,J,K) * workqm(I,J,K);
				dqdy_4d(I-il3,J-jl3,K-kl3,m) += worksy(I,J,K) * workqm(I,J,K);
				dqdz_4d(I-il3,J-jl3,K-kl3,m) += worksz(I,J,K) * workqm(I,J,K);
#endif
			}
		}

		Range I0(1,ni);
		Range J0(1,nj);
		Range K0(1,nk);

#ifdef EXPANDED
		#pragma omp for schedule(dynamic)nowait
		#pragma ivdep
		for(int k = 1; k <= nk; ++k) {
			for(int j = 1; j <= nj; ++j) {
				#pragma omp simd
				for(int i = 1; i <= ni; ++i) {
					Pworkqm[LOC3D(i,j,k)] = 1.0 / (  Pvol[LOC3D(i,j,k)] + Pvol[LOC3D(i-il1, j-jl1, k-kl1)] );
				}
			}
		}
#else
		workqm(I0,J0,K0) = 1.0 / (  vol(I0, J0, K0) + vol(I0-il1, J0-jl1, K0-kl1) );
#endif

		for ( int m = mst; m <= med; ++ m )
		{
#ifdef EXPANDED
			#pragma omp for schedule(dynamic)nowait
			#pragma ivdep
			for(int k = 1; k <= nk; ++k) {
				for(int j = 1; j <= nj; ++j) {
					#pragma omp simd
					for(int i = 1; i <= ni; ++i) {					
								Pdqdx_4d[LOC4D(i,j,k,m)] *= Pworkqm[LOC3D(i,j,k)];
								Pdqdy_4d[LOC4D(i,j,k,m)] *= Pworkqm[LOC3D(i,j,k)];
								Pdqdz_4d[LOC4D(i,j,k,m)] *= Pworkqm[LOC3D(i,j,k)];
					}
				}
			}
#else
			dqdx_4d(I0,J0,K0,m) *= workqm(I0,J0,K0);
			dqdy_4d(I0,J0,K0,m) *= workqm(I0,J0,K0);
			dqdz_4d(I0,J0,K0,m) *= workqm(I0,J0,K0);
#endif
		}

	// ¸Ã·½Ïò½çÃæÌÝ¶ÈÖµ±»¼ÆËã³öÀ´ºó£¬»áÓÃÓÚÕ³ÐÔÍ¨Á¿¼ÆËã£¬¸ÃÖµÊ¹ÓÃºóÏÂÒ»·½Ïò»áÖØÐÂ¸³0¼ÆËã
#undef LOC3D
#undef LOC4D
	}
}
	//----------------------------------------------------
	//ÒÔÏÂÎªÕýÈ·ÐÔ¶Ô±È²¿·Ö£¬²»¿ÉÐÞ¸Ä£¡
	//----------------------------------------------------
	end=rdtsc();
	elapsed= (end - start)/(F*Time);
	cout<<"The programe elapsed "<<elapsed<<setprecision(8)<<" s"<<endl;
	if(!preccheck(dqdx_4d,dqdy_4d,dqdz_4d))
		cout<<"Result check passed!"<<endl;
	return 0;
}
```

结果如下：

![](./vtune10.png)

CPU time = 1.03706s

时间比为： 83.6144s / 1.03706s = 80.62638613002141倍

此时可以发现上述问题果然存在并且已经解决了上述问题，于是此时重新使用vtune分析性能瓶颈：

![](./vtune11.png)

使用下述命令检查cache命中率：

![](./vtune12.png)

下面是结果：

![](./vtune13.png)

可以看到L1L2的命中率都足够高，于是此时根据analysis猜测应该与向量化有一定关系，而且因为向量化报告也显示因为内层向量化而外层不再向量化。

此时找到一篇关于有效进行向量化的论文（`FINE-TUNING VECTORIZATION AND MEMORY TRAFFIC ON INTEL XEON PHI COPROCESSORS: LU DECOMPOSITION OF SMALL MATRICES`），按照论文中描述的方法进行向量化：

文中所有方法如下：

![](./simd11.png)

首先提到在编译参数中加入-xHost，作用是使得。经过此处修改后发现程序`可以稳定跑进1s`：

![](./vtune15.png)

![](./vtune21.png)

文中提到的第一种优化方法是strength reduction，意思就是通过改写代码使得除法操作变为乘法，实际上查阅资料有：

因为计算机的运算法则为补码计算，因此对于同一组数据往往加法快于减法，乘法快于除法。但是这个需要将m调整为内循环，而原来的代码之前调整过顺序，会导致omp for速率下降，所以不想大改却没有效果，所以不尝试了。

不过这一次优化尝试提示从程序本身进行优化，这一次首先学习了从本身优化的一些方法：https://www.cnblogs.com/zzdbullet/p/9642466.html后进行优化：尝试使用循环展开：核心思想是通过每次循环计算两个元素从而减少循环次数，因为for i为内层循环，所以对for i循环展开（连续访问），但是还是比原来慢，拿vtune分析一下，发现并不能提高效率，比如最后两个循环，没有修改过，而其cpu time占比也没有变，分析是因为线程和向量化已经尽可能分割了循环计算，导致过细粒度反而会导致效率低，也同时说明瓶颈并不在循环计算的循环开销上。

而分析发现此时时间变长是因为cache命中率降低，说明cache优化已经不行了（不用再优化了）,**那么有关cache命中率优化的方法不再考虑**。

之前cache：

![](./vtune16.png)

之后cache:

![](./vtune17.png)

所以不用优化cache只能回来接着再次回到原论文进行优化向量化了，毕竟向量化只优化了1.5倍。

文中第二种方法：regularizing vectorization pattern不能用，因为不是方阵。第三种方法是alignment and aligned data hint，像这样加上编译时align就行：

![](./vtune18.png)

同时编译时声明数据无依赖（第四种方法：pointer disambiguation），之前使用过，这次在向量化下进行尝试，这一条并没有效果，可能编译器已做了优化。

下面文章对内存优化提出了一些方法（memory traffic tuning），首先看这一项：

![](./vtune19.png)

很明显看到红线这一项不够理想，因此使用一种文中提到交作loop titling的方法，具体方法一看就会，目的是数据重用提高局部性，也就是矩阵分块：

![](./vtune20.png)

将分块大小定为16，但是最后发现不行，因为这个方法破坏了数据依(赖性，最外层和最内层循环产生了数据依赖。

优化结果：

83.6144s / 0.917084s = 91.17419996423446倍

最后既然瓶颈就在循环次数上，化简结构合并循环有：

![](./vtune22.png)

优化结果：

83.6144s / 0.87535s = 95.52110584337693倍

## part2 系统组

1. VM setup

首先从下述网站选取一个镜像文件：

http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1810.iso

我选的是这一个：

http://mirrors.aliyun.com/centos/7.6.1810/isos/x86_64/CentOS-7-x86_64-DVD-1810.iso

接下来安装两个虚拟机并设置为70GB以上磁盘容量

![](./xt1.png)

因为centOS主要作为服务器的操作系统，所以一般是不用图形界面的。

下面安装Compilers/Libraries/Applications：

首先确保互联网连接：

详情参考：https://blog.csdn.net/demonwolfe/article/details/80299015

![](./tx1.png)

![](./tx2.png)

按照文章所述使用xftp将文件传入虚拟机：

详情参考：

https://blog.csdn.net/clean_water/article/details/53023308

![](./tx3.png)

此时已经可以使用新的yum安装依赖，于是做好所有准备工作所以来编译gcc：

详情参考：https://cloud.tencent.com/developer/article/1365651

（一般都是下了用xftp放进去）

在前面步骤完成后开始漫长的编译：

![](./tx4.png)

![](./tx5.png)

接下来`sudo make install`后重新建立软链接：

```shell
find / -name "libstdc++.so*" #找到自己的文件路径
#把libstdc++.socopy到/usr/lib64目录，类似下面的命令

cp /root/gcc-7.3.0/gcc-build-7.3.0/x86\_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.24 /usr/lib64

cd /usr/lib64 

rm -rf libstdc++.so.6 #删除原来的

ln -s libstdc++.so.6.0.24 libstdc++.so.6 #重新建立软连
```

此时一测试发现没变，于是又找了一篇文章：

https://www.jianshu.com/p/36f5d3524240

删除了旧gcc，链接了新gcc，最后测试为gcc-7.3.0（完成）：

编译gcc：

参考这里：https://www.cnblogs.com/davygeek/p/8018628.html

![](./tx6.png)

![](./tx7.png)

![](./tx8.png)

接下来安装intel parallel studio XE 2019

详情参考：https://blog.csdn.net/lk040384/article/details/88757558

找到路径并设置环境变量后运行icc证明完成：

![](./tx12.png)

接下来安装openMPI：

详情参考：https://blog.csdn.net/baidu_26646129/article/details/88425619

安装中途错误fortran compiler不work，查看发现/usr/lib64少了库libgfortran.so.4，将最新编译的libfortran.so.4.0.0链接过去即可:

![](./tx10.png)

验证如下，可以看到安装完成：

![](./tx11.png)

安装CUDA Toolkit:

详情参考：https://blog.csdn.net/gyx1549624673/article/details/86482743

安装时/tmp空间不够（默认1mb）：https://askubuntu.com/questions/199565/not-enough-space-on-tmp

安装成功并测试：

1)运行nvcc

![](./tx20.png)

2)编译samples例子 

进入到Samples安装目录，然后在该目录下终端输入make，发现可以编译成功，但是我们用的虚拟机没有设备，因此cudaGetDeviceCount为0(error : no CUDA-capable device is detected)：

![](./tx21.png)


安装CMake 3：

详情参考：https://blog.csdn.net/jiang_xinxing/article/details/77945478

![](./tx13.png)

2. Module

此时完成所有的setup后进行environment module的操作，首先要安装module，它是用来管理环境变量的一个工具。我们的任务是写一些modulefile来使得user可以加载上述环境。

详情参考：https://blog.csdn.net/jslove1997/article/details/80338370

https://blog.csdn.net/qq_41493990/article/details/81531411

1)首先安装Module:

yum install environment-modules

2)安装完成后，会默认在/usr/share/module目录下产生一系列文件夹，其中init中放了在各种系统下初始化这个程序的脚本，我的终端是bash，所以使用如下命令初始化。 
source /usr/share/module/init/bash 
加在.bashrc下

如图，安装完成：

![](./tx14.png)

3)编写文件

示范的modulefile如下：

```
#%Module1.0
module-whatis    Provides the Intel fortran and c compiler mkl 2015 environment for x86_64 systems
setenv           INTEL_LICENSE_FILE /opt/intel/licenses/all_intel_products.lic
#conflict        intel pathscale pgi gcc open64
setenv           CC icc
setenv           CXX icpc
setenv           FC ifort
setenv           F77 ifort
setenv           F90 ifort
setenv           F95 ifort
setenv           CFLAGS -O2
setenv           FFLAGS -O2
setenv           OFLAGS -O3
setenv           GFLAGS -g -O1
setenv           MKLROOT /opt/intel/mkl
prepend-path     PATH /opt/intel/composer_xe_2015/bin
prepend-path     MANPATH /opt/intel/composer_xe_2015/man/en_US
prepend-path     LD_RUN_PATH     /opt/intel/composer_xe_2015/lib/intel64:/opt/intel/composer_xe_2015/mkl/lib/intel64
prepend-path     LD_LIBRARY_PATH /opt/intel/composer_xe_2015/lib/intel64:/opt/intel/composer_xe_2015/mkl/lib/intel64
prepend-path     LIBRARY_PATH    /opt/intel/composer_xe_2015/lib/intel64:/opt/intel/composer_xe_2015/mkl/lib/intel64
```

4)运行文件

module-environment的使用方法。 

使用module load xxx命令后，系统就会在特定路径下去寻找名字为xxx的文件，然后按照该文件里面的内容来修改环境变量。 

这个特定的路径默认是在/usr/share/modules/modulefile和一些其他的地方。如果想修改或者说添加一些别的路径，可以通过在初始化文件中添加命令修改MODULEPATH来达到目的。 
在/usr/share/module/init/bash中最后一行添加 
export MODULEPATH=/YOUR/PATH/:$MODULEPATH 

module avail 显示可以使用的模块 
module load 加载模块 
module unload 卸载模块 
module list 显示已经加载的模块


通过提示，我们使用env2生成modulefiles，详情参考：

http://blog.chonor.cn/index.php/environment-module/

拿我们的intel.sh举例子就是：

先查看里面有什么：

![](./tx16.png)

然后按教程生成文件并放入/usr/share/Module/modulefiles：

![](./tx17.png)

以intel-1文件为例：

前面加入表示为Module文件：

```
#%Module 1.0
```

一看现在有了:

![](./tx18.png)

module load 该模块即可（完成）：

![](./tx19.png)


【参考】

+ SIMD Vectorization with OpenMP.PDF

+ OPENMP编程简要指南.pdf

+ 《以块为主序的二维数组的存储结构》

+ 《高阶精度CFD应用在天河2系统上的异构并行模拟与性能优化》

+ https://hgpu.org/papers/13441_20150209154931.pdf